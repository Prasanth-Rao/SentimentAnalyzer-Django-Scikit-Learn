{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "analysisModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmDgGiAo2Pra"
      },
      "source": [
        "## **Generate a training Model for Sentiment Analysis using Scikit-learn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unWyKGQu2B-1"
      },
      "source": [
        "**Importing Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g8_3Vj-klqx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBFEgufv2wHB"
      },
      "source": [
        "**Import and download required nltk packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2j_CEmpKCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151aba5c-209a-4e35-e3c6-169e8860a0c2"
      },
      "source": [
        "import nltk\n",
        "# Run this cell to download required packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJW98JZ93Ewn"
      },
      "source": [
        "**Text Preprocessing Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aui1B1IBmSiC"
      },
      "source": [
        "class TextPreprocess:\n",
        "  port_stemmer = PorterStemmer()\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  def __int__(self):\n",
        "    pass\n",
        "  \n",
        "  def to_lower(self,text):\n",
        "    text_lower = text.lower()\n",
        "    return text_lower\n",
        "  \n",
        "  def remove_numbers(self,text):\n",
        "    output = ''.join(word for word in text if not word.isdigit())\n",
        "    return output\n",
        "  \n",
        "  def remove_punctuation(self,text):\n",
        "    output = ''.join(c for c in text if c not in punctuation)\n",
        "    return output\n",
        "  \n",
        "  def remove_Tags(self,text):\n",
        "    cleaned_text = re.sub('<[^<]+?>', '', text)\n",
        "    return cleaned_text\n",
        "  \n",
        "  def remove_stopwords(self,sentence):\n",
        "    stop_words = stopwords.words('english')\n",
        "    output = ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
        "    return output\n",
        "\n",
        "  def tokenize_word(self,text):\n",
        "    output = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    return output\n",
        "\n",
        "  def stemmer(self, tokenList):\n",
        "    output = [self.port_stemmer.stem(word) for word in tokenList]\n",
        "    return output\n",
        "\n",
        "  def lemmatizer(self,tokenList):\n",
        "    lemmatized_word = [self.wordnet_lemmatizer.lemmatize(word) for word in tokenList]\n",
        "    output = \" \".join(lemmatized_word)\n",
        "    return output\n",
        "\n",
        "  def normalizer(self, text):\n",
        "    lowerText = self.to_lower(text)\n",
        "    clean_text = self.remove_punctuation(lowerText)\n",
        "    clean_text = self.remove_numbers(clean_text)\n",
        "    clean_text = self.remove_stopwords(clean_text)\n",
        "    wordTokens = self.tokenize_word(clean_text)\n",
        "    stemmedList = self.stemmer(wordTokens)\n",
        "    lemmatizedText = self.lemmatizer(stemmedList)\n",
        "\n",
        "    return lemmatizedText\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmo1ltfg3UMJ"
      },
      "source": [
        "**loading csv dataset into pandas dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YlhFxcck_YW"
      },
      "source": [
        "df = pd.read_csv('/content/Tweets.csv') # change csv path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDwMgU_Zoj_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "9354fd22-b171-4a66-dafe-6c406a2b1f75"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eevkHreh3jOn"
      },
      "source": [
        "**Applying Preprocess class for the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdEVQr88AncL"
      },
      "source": [
        "textProcess = TextPreprocess()\n",
        "\n",
        "df = shuffle(df)\n",
        "y = df['airline_sentiment'].astype(str)\n",
        "x = df.text.apply(textProcess.normalizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPWhzbHy3xv8"
      },
      "source": [
        "**Vectorizing the Text column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9vEYt0UA3EY"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "x_vectorized = vectorizer.fit_transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UZMlWKpBIqJ"
      },
      "source": [
        "train_x,val_x,train_y,val_y = train_test_split(x_vectorized,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_h7B3OB34eJ"
      },
      "source": [
        "**Applying Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7K0MMMOBMh8"
      },
      "source": [
        "regressor = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
        "model = regressor.fit(train_x, train_y)\n",
        "regression_accuracy = regressor.score(val_x, val_y) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjurRa7FBY7u"
      },
      "source": [
        "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "gs_clf = GridSearchCV(model, params, n_jobs=1, cv=5)\n",
        "gs_clf = gs_clf.fit(train_x, train_y)\n",
        "model = gs_clf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Hb7xtNJev8c",
        "outputId": "53cadb68-e1f7-4faa-92eb-a8e24ae6cf5e"
      },
      "source": [
        "val_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3660,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqEHio764EqM"
      },
      "source": [
        "**Checking predictions for previously trained Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtJmnhT4Be3K"
      },
      "source": [
        "y_pred = model.predict(val_x)\n",
        "\n",
        "_f1 = f1_score(val_y, y_pred, average='micro')\n",
        "_confusion = confusion_matrix(val_y, y_pred)\n",
        "__precision = precision_score(val_y, y_pred, average='micro')\n",
        "_recall = recall_score(val_y, y_pred, average='micro')\n",
        "_model_accuracy = accuracy_score(val_y, y_pred) *100\n",
        "_statistics = {'f1_score': _f1,\n",
        "               'confusion_matrix': _confusion,\n",
        "               'precision': __precision,\n",
        "               'recall': _recall,\n",
        "               'accuracy': _model_accuracy\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhr47HUzB0JZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "35a14c3f-6b14-488b-a0c2-ff5af8dfed4d"
      },
      "source": [
        "print(_statistics)\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ac339535402>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_statistics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_statistics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovbtYXa74VCD"
      },
      "source": [
        "**Testing Prediction Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6exE7qRB5Nj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b864f093-6278-4b8b-d648-80e7b9b7768d"
      },
      "source": [
        "text = [\"This love this to the core\"]\n",
        "test_feature = vectorizer.transform(text)\n",
        "predicted = model.predict(test_feature)\n",
        "_model_accuracy = accuracy_score(val_y, predicted) *100\n",
        "_model_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fb0d148b5491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_model_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0m_model_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3660, 1]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V059qRZt4bej"
      },
      "source": [
        "**Dumping the model into a Pickle format to use in the project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLAAYpDiCCuf"
      },
      "source": [
        "import pickle\n",
        "model_pickle = {'vectorizer': vectorizer, 'model': model, 'actual_data': val_y}\n",
        "pickle.dump(model_pickle, open('models'+\".p\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFbDQ_sfEZwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}